{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Propagation - Tutorial 1\n",
    "## Standard Target Propagation\n",
    "\n",
    "A simple implementation of target propagation to confirm that it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "from tools.modules import Sign, Stochastic, Clamp\n",
    "from tools.training import train, classify\n",
    "from tools import training\n",
    "from tools.learners.target_prop import TargetPropLearner, AlternateTraining\n",
    "from tools.learners.target_prop import BaselineLearner1\n",
    "from functools import partial\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps\n",
    "\n",
    "1) Create each layer (AutoencoderLearner)\n",
    "2) Create the TargetPropLearner\n",
    "3) Run the training on the baseline\n",
    "4) Run the training on the target propagation learner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "training_dataset = FashionMNIST(\n",
    "    '../../Datasets/',\n",
    "    transform=transform, download=True\n",
    ")\n",
    "\n",
    "testing_dataset = FashionMNIST(\n",
    "    '../../Datasets/', train=False,\n",
    "    transform=transform, download=True\n",
    ")\n",
    "\n",
    "baseline_loss= {}\n",
    "baseline_class= {}\n",
    "tp_loss = {}\n",
    "tp_class = {}\n",
    "tp_alt_loss = {}\n",
    "tp_alt_class = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Learner\n",
    "\n",
    "Train baseline learners on \n",
    " - LeakyReLU \n",
    " - Sign\n",
    " - Stochastic\n",
    "\n",
    "Use straight-through-estimators for the latter two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act = 'stochastic'\n",
    "\n",
    "\n",
    "# if act == 'leaky_relu':\n",
    "# elif act == 'sign':\n",
    "#     activation = partial(Sign, True)\n",
    "# elif act == 'stochastic':\n",
    "#     activation = partial(Stochastic, True, True)\n",
    "\n",
    "activation = nn.LeakyReLU\n",
    "\n",
    "learner = BaselineLearner1(\n",
    "    784, 300, 300, 300, 10, activation=activation\n",
    ")\n",
    "baseline_loss[act] = train(learner, training_dataset, 20, device='cpu')\n",
    "baseline_class[act] = classify(learner, testing_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TargetPropLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "act = 'leaky_relu'\n",
    "\n",
    "for act in ['leaky_relu', 'sign', 'stochastic']:\n",
    "    print('Activation: ', act)\n",
    "    if act == 'leaky_relu':\n",
    "        activation = nn.LeakyReLU\n",
    "        in_act = None\n",
    "    elif act == 'sign':\n",
    "        activation = nn.Tanh\n",
    "        in_act = partial(Sign, False)\n",
    "    elif act == 'stochastic':\n",
    "        in_act = partial(Stochastic, False, False)\n",
    "        activation = nn.Sigmoid\n",
    "\n",
    "    learner = TargetPropLearner(\n",
    "        784, 300, 300, 300, 10, dropout_p=0.1, act=activation, out_x_lr=1e-3\n",
    "    )\n",
    "    alternator = AlternateTraining(learner, 1, 1)\n",
    "\n",
    "    tp_loss[act] = train(learner, training_dataset, 20, device='cpu', callback=None)\n",
    "    tp_class[act] = classify(learner, testing_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "act = 'leaky_relu'\n",
    "\n",
    "for act in ['leaky_relu', 'sign', 'stochastic']:\n",
    "\n",
    "    if act == 'leaky_relu':\n",
    "        activation = nn.LeakyReLU\n",
    "        in_act = None\n",
    "    elif act == 'sign':\n",
    "        activation = nn.Tanh\n",
    "        in_act = partial(Sign, False)\n",
    "    elif act == 'stochastic':\n",
    "        in_act = partial(Stochastic, False, False)\n",
    "        activation = nn.Sigmoid\n",
    "\n",
    "    learner = TargetPropLearner(\n",
    "        784, 300, 300, 300, 10, act=activation, reverse_act=activation, in_act=in_act, out_x_lr=1., use_norm=True\n",
    "    )\n",
    "\n",
    "    alternator = AlternateTraining(learner, 1, 4)\n",
    "\n",
    "    tp_alt_loss[act] = train(learner, training_dataset, 20, device='cpu', callback=alternator)\n",
    "    tp_alt_class[act] = classify(learner, testing_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(baseline_loss.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.plot_loss_line(\n",
    "    [baseline_loss['stochastic'], tp_loss['leaky_relu'], tp_loss['sign'], tp_loss['stochastic']], \n",
    "    ['Baseline', 'Target Prop - Leaky ReLU', 'Target Prop - Sign', 'Target Prop - Stochastic'], \n",
    "    'Training Loss', save_file='images/t2x1_target_prop_2024_10_3_1.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    baseline_class['stochastic'], tp_class['leaky_relu'], \n",
    "    tp_class['sign'], tp_class['stochastic']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "losses = {'target1': target_loss, 'target2': target_loss2}\n",
    "\n",
    "with open('results/t2x1_loss_results1.pkl', 'wb') as file:\n",
    "    pickle.dump(losses, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
