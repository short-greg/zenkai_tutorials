{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Propagation - Tutorial 1\n",
    "## Standard Target Propagation\n",
    "\n",
    "A simple implementation of target propagation to confirm that it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import FashionMNIST, CIFAR10\n",
    "from torchvision import transforms\n",
    "\n",
    "from tools.modules import Sign, Stochastic, Clamp\n",
    "from tools.training import train, classify\n",
    "from tools import training\n",
    "from tools.learners.target_propv2 import TargetPropLearner, AlternateTraining\n",
    "from tools.learners.target_propv2 import BaselineLearner1\n",
    "from functools import partial\n",
    "from torch import nn\n",
    "\n",
    "from tools.training import train, classify\n",
    "import zenkai\n",
    "from functools import partial\n",
    "\n",
    "from tools.learners.target_propv2 import BaselineLearner1, select_act, LinearTPLearner, TPAltStepTheta, DiffTPStepX, TPStepTheta, TPStepX, DeepLinearTPLearner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps\n",
    "\n",
    "1) Create each layer (AutoencoderLearner)\n",
    "2) Create the TargetPropLearner\n",
    "3) Run the training on the baseline\n",
    "4) Run the training on the target propagation learner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "training_dataset = CIFAR10(\n",
    "    '../../Datasets/',\n",
    "    transform=transform, download=True\n",
    ")\n",
    "\n",
    "testing_dataset = CIFAR10(\n",
    "    '../../Datasets/', train=False,\n",
    "    transform=transform, download=True\n",
    ")\n",
    "import math\n",
    "k = math.prod(testing_dataset[0][0].shape)\n",
    "\n",
    "losses = {}\n",
    "classifications = {}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(predictor: str, activation: str) -> str:\n",
    "    return f'{predictor}_{activation}'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Learner\n",
    "\n",
    "Train baseline learners on \n",
    " - LeakyReLU \n",
    " - Sign\n",
    " - Stochastic\n",
    "\n",
    "Use straight-through-estimators for the latter two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act = 'leaky_relu'\n",
    "activation = nn.LeakyReLU\n",
    "\n",
    "learner = BaselineLearner1(\n",
    "    k, 300, 300, 300, 10, activation=activation\n",
    ")\n",
    "\n",
    "key = 'Baseline - LeakyReLU'\n",
    "losses[key], epoch_results = train(learner, training_dataset, 40, device='cpu')\n",
    "classifications[key] = classify(learner, testing_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "print(np.mean(epoch_results['loss']))\n",
    "\n",
    "# Fashion MNIST 0.39\n",
    "# CIFAR 1.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# losses = {'baseline': baseline_loss}\n",
    "\n",
    "# with open('results/t2x1_loss_cifar_results2.pkl', 'wb') as file:\n",
    "#     pickle.dump(losses, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TargetPropLearner\n",
    "\n",
    "Train target propagation learners using \"LeakyReLU\", \"Sign\", and \"Stochastic\" activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "epoch_results = {}\n",
    "\n",
    "# for act in ['leaky_relu', 'sign', 'stochastic']:\n",
    "for act, rec_weight in [\n",
    "    ('leaky_relu', 1.0),\n",
    "    ('leaky_relu', None),\n",
    "    ('sign', 1.0),\n",
    "    ('sign', None),\n",
    "    ('stochastic', 1.0),\n",
    "    ('stochastic', None),\n",
    "]:\n",
    "    print('Activation: ', act)\n",
    "    key, act, in_act = select_act(act)\n",
    "\n",
    "    step_x = TPStepX.factory()\n",
    "\n",
    "    step_theta = TPStepTheta.factory(\n",
    "        zenkai.NNLoss('MSELoss', 'mean'),\n",
    "        zenkai.NNLoss('MSELoss', 'mean'),\n",
    "        1e-3, True, True, rec_weight, 1.0\n",
    "    )\n",
    "\n",
    "    i = 0\n",
    "    out_x_lr = 0.1\n",
    "\n",
    "    learner = DeepLinearTPLearner(\n",
    "        k, 300, 300, 300, 10, step_theta, step_x, 1e-3, \n",
    "        act, act, in_act, True, True, 1e-3, 0.1, None\n",
    "    )\n",
    "\n",
    "    losses[key], epoch_results[act] = train(learner, training_dataset, 20, device='cpu', callback=None)\n",
    "    classifications[key] = classify(learner, testing_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "act = 'leaky_relu'\n",
    "\n",
    "epoch_results = {}\n",
    "\n",
    "# for act in ['leaky_relu', 'sign', 'stochastic']:\n",
    "for act in ['sign']:\n",
    "    print('Activation: ', act)\n",
    "    if act == 'leaky_relu':\n",
    "        key = 'Target Prop - Leaky ReLU'\n",
    "        activation = nn.LeakyReLU\n",
    "        in_act = None\n",
    "    elif act == 'sign':\n",
    "        key = 'Target Prop - TanH'\n",
    "        activation = nn.Tanh\n",
    "        in_act = partial(Sign, False)\n",
    "    elif act == 'stochastic':\n",
    "        key = 'Target Prop - Stochastic'\n",
    "        in_act = partial(Stochastic, False, False)\n",
    "        activation = nn.Sigmoid\n",
    "    else:\n",
    "        raise ValueError(f'Cannot use act {act}')\n",
    "\n",
    "    learner = TargetPropLearner(\n",
    "        784, 300, 300, 300, 10, dropout_p=0.1, act=activation, in_act=in_act, out_x_lr=1e-3\n",
    "    )\n",
    "    # alternator = AlternateTraining(learner, 1, 1)\n",
    "    losses[key], epoch_results[key] = train(learner, training_dataset, 2, device='cpu', callback=None)\n",
    "    classifications[key] = classify(learner, testing_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(epoch_results['leaky_relu']['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "keys = list(losses.keys())\n",
    "values = list(losses.values())\n",
    "\n",
    "training.plot_loss_line(\n",
    "    values, keys, \n",
    "    'Training Loss', save_file='images/t2x1_target_prop_2024_10_3_1.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification = '\\n'.join(f'{k}: {v}' for k, v in classifications.items())\n",
    "print(classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('results/t2x1_loss_results1.pkl', 'wb') as file:\n",
    "    pickle.dump(losses, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
