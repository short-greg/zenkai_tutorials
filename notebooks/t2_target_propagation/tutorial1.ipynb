{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Propagation - Tutorial 1\n",
    "## Standard Target Propagation\n",
    "\n",
    "A simple implementation of target propagation to confirm that it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "from tools.modules import Sign, Stochastic, Clamp\n",
    "from tools.training import train, classify\n",
    "from tools import training\n",
    "from tools.learners.target_prop import TargetPropLearner, AlternateTraining\n",
    "from tools.learners.target_prop import BaselineLearner1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps\n",
    "\n",
    "1) Create each layer (AutoencoderLearner)\n",
    "2) Create the TargetPropLearner\n",
    "3) Run the training on the baseline\n",
    "4) Run the training on the target propagation learner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "training_dataset = FashionMNIST(\n",
    "    '../../Datasets/',\n",
    "    transform=transform, download=True\n",
    ")\n",
    "\n",
    "testing_dataset = FashionMNIST(\n",
    "    '../../Datasets/', train=False,\n",
    "    transform=transform, download=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = BaselineLearner1(\n",
    "    784, 300, 300, 300, 10\n",
    ")\n",
    "baseline_loss = train(learner, training_dataset, 10, device='cpu')\n",
    "classify(learner, testing_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TargetPropLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = TargetPropLearner(\n",
    "    784, 300, 300, 300, 10, dropout_p=0.0, out_x_lr=None\n",
    ")\n",
    "\n",
    "alternator = AlternateTraining(learner, 4, 8)\n",
    "\n",
    "\n",
    "target_loss = train(learner, training_dataset, 50, device='cpu', callback=alternator)\n",
    "\n",
    "classify(learner, testing_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "learner = TargetPropLearner(\n",
    "    784, 300, 300, 300, 10, act=nn.Tanh, reverse_act=nn.Tanh, in_act=Sign, out_x_lr=1.0, use_norm=False\n",
    ")\n",
    "\n",
    "\n",
    "target_loss = train(learner, training_dataset, 50, device='cpu')\n",
    "\n",
    "classify(learner, testing_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "print('Create learner')\n",
    "\n",
    "\n",
    "learner = TargetPropLearner(\n",
    "    784, 300, 300, 300, 10, nn.Tanh, nn.Tanh, lambda: torch.sign, dropout_p=0.5, out_x_lr=None\n",
    ")\n",
    "\n",
    "alternator = AlternateTraining(learner, 1, 4)\n",
    "\n",
    "hard_target_loss = train(learner, training_dataset, 20, device='cpu', callback=alternator)\n",
    "\n",
    "classify(learner, testing_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "print('Create learner')\n",
    "learner = TargetPropLearner(\n",
    "    784, 300, 300, 300, 10, nn.Sigmoid, nn.Sigmoid, Stochastic, dropout_p=0.25, out_x_lr=1.0\n",
    ")\n",
    "\n",
    "alternator = AlternateTraining(learner, 1, 4)\n",
    "\n",
    "hard_target_loss = train(learner, training_dataset, 20, device='cpu')\n",
    "\n",
    "classify(learner, testing_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.plot_loss_line(\n",
    "    [baseline_loss, target_loss, hard_target_loss], \n",
    "    ['Baseline', 'Target Prop - Leaky ReLU', 'Target Prop - Sign'], \n",
    "    'Training Loss', save_file='images/t2x1_target_prop.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "losses = {'target1': target_loss, 'target2': target_loss2}\n",
    "\n",
    "with open('results/t2x1_loss_results1.pkl', 'wb') as file:\n",
    "    pickle.dump(losses, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
