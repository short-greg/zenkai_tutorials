{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Propagation - Tutorial 1\n",
    "## Standard Target Propagation\n",
    "\n",
    "A simple implementation of target propagation to confirm that it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import FashionMNIST, CIFAR10\n",
    "from torchvision import transforms\n",
    "\n",
    "from tools.modules import Sign, Sampler, Clamp\n",
    "from tools.training import train, classify\n",
    "from tools import training\n",
    "from tools.learners.target_propv2 import DeepLinearTPLearner\n",
    "from tools.learners.target_propv2 import BaselineLearner1\n",
    "from functools import partial\n",
    "from torch import nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tools.training import train, classify\n",
    "import zenkai\n",
    "from functools import partial\n",
    "\n",
    "from tools.learners.target_propv2 import BaselineLearner1, select_act, LinearTPLearner, TPAltStepTheta, DiffTPStepX, TPStepTheta, TPStepX, DeepLinearTPLearner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps\n",
    "\n",
    "1) Create each layer (AutoencoderLearner)\n",
    "2) Create the TargetPropLearner\n",
    "3) Run the training on the baseline\n",
    "4) Run the training on the target propagation learner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "training_dataset = CIFAR10(\n",
    "    '../../Datasets/',\n",
    "    transform=transform, download=True\n",
    ")\n",
    "\n",
    "testing_dataset = CIFAR10(\n",
    "    '../../Datasets/', train=False,\n",
    "    transform=transform, download=True\n",
    ")\n",
    "import math\n",
    "k = math.prod(testing_dataset[0][0].shape)\n",
    "\n",
    "losses = {}\n",
    "classifications = {}\n",
    "\n",
    "\n",
    "epoch_results = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(predictor: str, activation: str) -> str:\n",
    "    return f'{predictor}_{activation}'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Learner\n",
    "\n",
    "Train baseline learners on \n",
    " - LeakyReLU \n",
    " - Sign\n",
    " - Stochastic\n",
    "\n",
    "Use straight-through-estimators for the latter two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act = 'leaky_relu'\n",
    "activation = nn.LeakyReLU\n",
    "\n",
    "learner = BaselineLearner1(\n",
    "    k, 300, 300, 300, 10, activation=activation\n",
    ")\n",
    "\n",
    "key = 'Baseline - LeakyReLU'\n",
    "losses[key], epoch_results[key] = train(learner, training_dataset, 40, device='cpu')\n",
    "classifications[key] = classify(learner, testing_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "\n",
    "# print(np.mean(epoch_results['loss']))\n",
    "\n",
    "# Fashion MNIST 0.39\n",
    "# CIFAR 1.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# losses = {'baseline': baseline_loss}\n",
    "\n",
    "# with open('results/t2x1_loss_cifar_results2.pkl', 'wb') as file:\n",
    "#     pickle.dump(losses, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TargetPropLearner\n",
    "\n",
    "Train target propagation learners using \"LeakyReLU\", \"Sign\", and \"Stochastic\" activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "append = '3layer'\n",
    "\n",
    "# for act in ['leaky_relu', 'sign', 'stochastic']:\n",
    "for act, rec_weight in [\n",
    "    # ('leaky_relu', None),\n",
    "    # ('binary', None),\n",
    "    # 'sampler', None),\n",
    "    # ('clamp', None),\n",
    "    ('sign', None),\n",
    "    # ('sigmoid', None)\n",
    "    # ('leaky_relu', 0.1),\n",
    "    # ('sign', 0.1),\n",
    "    # ('stochastic', 1.0),\n",
    "    # ('stochastic', None),\n",
    "]:\n",
    "    print('Activation: ', act)\n",
    "\n",
    "    key, act, in_act = select_act(act, rec_weight)\n",
    "\n",
    "    key += f'_{append}'\n",
    "    step_x = TPStepX.factory()\n",
    "\n",
    "    step_theta = TPAltStepTheta.factory(\n",
    "        zenkai.NNLoss('MSELoss', 'mean'),\n",
    "        zenkai.NNLoss('MSELoss', 'mean'),\n",
    "        1e-4, 1e-4, True, True, rec_weight, 1.0, # reg=1e0\n",
    "    )\n",
    "\n",
    "    i = 0\n",
    "    out_x_lr = 1.0\n",
    "    print(k)\n",
    "    learner = DeepLinearTPLearner(\n",
    "        k, 200, 200, 200, 10, step_theta, step_x, out_x_lr,\n",
    "        act, act, in_act, True, False, 1e-4, dropout_p=None, \n",
    "        gaussian_noise=0.1, share_weights=False\n",
    "    )\n",
    "\n",
    "    losses[key], epoch_results[key] = train(\n",
    "        learner, training_dataset, 40, \n",
    "        device='cpu', callback=None, batch_size=128\n",
    "    )\n",
    "    classifications[key] = classify(learner, testing_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(epoch_results['leaky_relu']['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "keys = list(losses.keys())\n",
    "values = list(losses.values())\n",
    "\n",
    "training.plot_loss_line(\n",
    "    values, keys, \n",
    "    'Training Loss', save_file='images/t2x1_target_prop_2024_10_3_1.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification = '\\n'.join(f'{k}: {v}' for k, v in classifications.items())\n",
    "print(classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('results/t2x1_loss_results1.pkl', 'wb') as file:\n",
    "    pickle.dump(losses, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
