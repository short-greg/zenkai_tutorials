{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Propagation - Tutorial 2\n",
    "## Differance Target Propagation\n",
    "\n",
    "Tutorial on an improvement of target propagation that uses the difference between the propagated target and the reconstructed input. Also, will evaluate different learning rates for the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import FashionMNIST, CIFAR10, MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "from tools.training import train, classify, plot_loss_line\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "\n",
    "from tools.modules import Sign, Sampler, Clamp\n",
    "\n",
    "from tools.training import train, classify, plot_loss_line\n",
    "import zenkai\n",
    "from functools import partial\n",
    "\n",
    "from tools.learners.target_propv2 import (\n",
    "    BaselineLearner1, select_act, TPMultAltStepTheta, \n",
    "    NullTPStepTheta, LinearTPLearner, TPAltStepTheta, \n",
    "    DiffTPStepX, TPStepTheta, TPStepX, DeepLinearTPLearner,\n",
    "    filter_acts, TPRegAltStepTheta\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps\n",
    "\n",
    "1) Create each layer (Difference AutoencoderLearner)\n",
    "2) Create the DiffTargetPropLearner\n",
    "3) Run the training on the DiffTargetProp learner using varying learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "\n",
    "training_dataset = FashionMNIST(\n",
    "    '../../Datasets/',\n",
    "    transform=transform, \n",
    "    download=True\n",
    ")\n",
    "testing_dataset = FashionMNIST(\n",
    "    '../../Datasets/',\n",
    "    transform=transform, \n",
    "    download=True, \n",
    "    train=False\n",
    ")\n",
    "\n",
    "losses = {}\n",
    "classifications = {}\n",
    "\n",
    "epoch_results = {}\n",
    "import math\n",
    "k = math.prod(testing_dataset[0][0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "append = '3layer'\n",
    "for key, act in [\n",
    "    ('leaky_relu', nn.LeakyReLU), \n",
    "    ('clamp', Clamp), \n",
    "    # ('sigmoid', nn.Sigmoid),\n",
    "    ('sign', partial(Sign, True))\n",
    "]:\n",
    "\n",
    "    # act = 'leaky_relu'\n",
    "    # activation = nn.LeakyReLU\n",
    "    # activation = partial(nn.LeakyReLU, 0, 1, False)\n",
    "\n",
    "    pred = 'baseline'\n",
    "\n",
    "    print(act)\n",
    "    learner = BaselineLearner1(\n",
    "        k, 200, 200, 200, 10, activation=act, lr=1e-3, dropout_p=0.1\n",
    "    )\n",
    "    cur_key = f'{pred}_{key}_{append}'\n",
    "    losses[cur_key], epoch_results[cur_key] = train(learner, training_dataset, 40, device='cpu')\n",
    "    classifications[cur_key] = classify(learner, testing_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "it = 0\n",
    "\n",
    "\n",
    "class Callback(object):\n",
    "\n",
    "    def __init__(self, learner, total_epochs: int, cycle: int=10):\n",
    "\n",
    "        self.learner = learner\n",
    "        self.total_epochs = total_epochs\n",
    "        self.it = 0\n",
    "        self.temperature = 0.0\n",
    "        self.increment = 1.0\n",
    "        self.epoch = 0\n",
    "        self.cycle = cycle\n",
    "\n",
    "    def reset(self):\n",
    "\n",
    "        self.it = 0\n",
    "        self.epoch = 0\n",
    "\n",
    "    def __call__(self, n_epochs, n_iterations, total_iterations):\n",
    "        \n",
    "        # if n_epochs > self.epoch:\n",
    "\n",
    "        #     for act in filter_acts(learner, Sampler):\n",
    "        #         act.temperature = min(\n",
    "        #             act.temperature + self.increment, 1.0\n",
    "        #         )\n",
    "        #     self.epoch = n_epochs\n",
    "        self.it = (self.it + 1) % self.cycle\n",
    "        if self.it == (self.cycle // 2):\n",
    "        # if self.it == self.cycle: # // 2):\n",
    "            for step_theta in learner.step_thetas():\n",
    "                step_theta.train_predictor = True\n",
    "                step_theta.train_reconstruction = True\n",
    "        elif self.it == 0:\n",
    "            for step_theta in learner.step_thetas():\n",
    "                step_theta.train_predictor = False\n",
    "                step_theta.train_reconstruction = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# epoch_results = {}\n",
    "\n",
    "append = '3layer'\n",
    "\n",
    "# for act in ['leaky_relu', 'sign', 'stochastic']:\n",
    "for act, rec_weight in [\n",
    "    # ('leaky_relu', None),\n",
    "    # ('binary', None),\n",
    "    # 'sampler', None),\n",
    "    # ('clamp', None),\n",
    "    ('sign', None),\n",
    "    # ('sigmoid', None)\n",
    "    # ('leaky_relu', 0.1),\n",
    "    # ('sign', 0.1),\n",
    "    # ('stochastic', 1.0),\n",
    "    # ('stochastic', None),\n",
    "]:\n",
    "    print('Activation: ', act)\n",
    "\n",
    "    key, act, in_act = select_act(act, rec_weight)\n",
    "\n",
    "    key += f'_{append}'\n",
    "    step_x = DiffTPStepX.factory(lr=1.0)\n",
    "\n",
    "    step_theta = TPAltStepTheta.factory(\n",
    "        zenkai.NNLoss('MSELoss', 'mean'),\n",
    "        zenkai.NNLoss('MSELoss', 'mean'),\n",
    "        1e-4, 1e-4, True, True, rec_weight, 1.0, # reg=1e0\n",
    "    )\n",
    "\n",
    "    i = 0\n",
    "    out_x_lr = 1.0\n",
    "    print(k)\n",
    "    learner = DeepLinearTPLearner(\n",
    "        k, 200, 200, 200, 10, step_theta, step_x, out_x_lr,\n",
    "        act, act, in_act, True, False, 1e-4, dropout_p=None, \n",
    "        gaussian_noise=0.1, share_weights=False\n",
    "    )\n",
    "\n",
    "    callback = Callback(\n",
    "        learner, 15, 2\n",
    "    )\n",
    "\n",
    "    losses[key], epoch_results[key] = train(\n",
    "        learner, training_dataset, 40, \n",
    "        device='cpu', callback=callback, batch_size=128\n",
    "    )\n",
    "    classifications[key] = classify(learner, testing_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(losses.keys()))\n",
    "\n",
    "sub_losses = {\n",
    "    k: v\n",
    "    for k, v in losses.items() if k not in ('Target Prop - Leaky ReLU - 0.1', 'TargetProp - TanH - 0.1', 'Target Prop - Stochastic')\n",
    "}\n",
    "print(list(sub_losses.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def moving_average(a, n=3):\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "    \n",
    "\n",
    "sub_loss_moving = {}\n",
    "\n",
    "for k, v in sub_losses.items():\n",
    "    if k == 'TargetProp - TanH':\n",
    "        k = 'TargetProp - Sign'\n",
    "\n",
    "    sub_loss_moving[k] = moving_average(v, 10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_line(\n",
    "    list(sub_loss_moving.values()), list(sub_loss_moving.keys()), \n",
    "    'Training Loss', save_file='images/t2x2_diff_target_prop_3layer_2024_12_17_1.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "all_results = {\n",
    "    'losses': losses,\n",
    "    'epoch_results': epoch_results,\n",
    "    'classifications': classifications\n",
    "}\n",
    "\n",
    "with open('results/t2x2_3layer_12_17_2024.pkl', 'wb') as file:\n",
    "    pickle.dump(all_results, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for k, v in all_results['epoch_results'].items():\n",
    "    # if k in ['Target Prop - Leaky ReLU_3layer', 'Target Prop - Clamp_3layer', 'TargetProp - Sign_3layer']:\n",
    "    print(v.keys())\n",
    "    for k2, v2 in v.items():\n",
    "        if k2 in ['loss']:\n",
    "\n",
    "            print(k, k2, np.var(v2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('results/t2x2_2layer_12_16_2024.pkl', 'rb') as file:\n",
    "    epoch_results = pickle.load(file)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(epoch_results['Target Prop - Leaky ReLU'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "list(epoch_results['Target Prop - Leaky ReLU'].keys())\n",
    "\n",
    "\n",
    "sims = {}\n",
    "for k, v in epoch_results.items():\n",
    "    if k == 'baseline_leaky_relu':\n",
    "        continue\n",
    "    for sim in ['0_x_sim', '1_x_sim', '2_x_sim']:\n",
    "        print(list(v.keys()))\n",
    "        sims[f'{k}_{sim}'] = np.mean(v[sim])\n",
    "\n",
    "sims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't propagate back reconstruction loss\n",
    "\n",
    "# learner = DiffTargetPropLearner(\n",
    "#     784, 300, 300, 300, 10, x_lr=0.5, act=torch.nn.LeakyReLU,\n",
    "#     reverse_act=torch.nn.LeakyReLU\n",
    "# )\n",
    "\n",
    "# training_losses = train(learner, training_dataset, 20)\n",
    "# classifications = classify(learner, testing_dataset)\n",
    "# print(classifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner = DiffTargetPropLearner(\n",
    "#     784, 300, 300, 300, 10, x_lr=0.1, act=torch.nn.LeakyReLU,\n",
    "#     reverse_act=torch.nn.LeakyReLU\n",
    "# )\n",
    "\n",
    "# training_losses2 = train(learner, training_dataset, 20)\n",
    "# classifications2 = classify(learner, testing_dataset)\n",
    "# print(classifications2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learner = DiffTargetPropLearner(\n",
    "    784, 300, 300, 300, 10, x_lr=0.5, act=lambda: torch.sign, reverse_act=nn.Sigmoid, targetf=torch.sign\n",
    ")\n",
    "\n",
    "train(learner, training_dataset, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# learner = DiffTargetPropLearner(\n",
    "#     784, 300, 300, 300, 10, x_lr=x_lr, \n",
    "#     act=torch.nn.LeakyReLU,\n",
    "#     reverse_act=torch.nn.LeakyReLU\n",
    "# )\n",
    "\n",
    "# train(learner, training_dataset, 100)\n",
    "# classify(learner, testing_dataset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
