{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro - Tutorial 1\n",
    "## Basics of Zenkai\n",
    "\n",
    "A simple tutorial for Zenkai to show how to build a basic learning machine that implements backprop and also expand upon that by adding loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import initialize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps\n",
    "\n",
    "1) Create standard network \n",
    "2) Create the STENetwork\n",
    "3) Create an alternative network that does looping for each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tools import training, evaluate\n",
    "from tools.learners import intro\n",
    "\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision import transforms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "training_dataset = FashionMNIST(\n",
    "    '../../Datasets/',\n",
    "    transform=transform, download=True\n",
    ")\n",
    "\n",
    "testing_dataset = FashionMNIST(\n",
    "    '../../Datasets/', train=False,\n",
    "    transform=transform, download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard network\n",
    "\n",
    "This network effectively implements a network using backprop with Zenkai for demonstration purposes. In general it would not make sense to do this unless you want to connect a \"standard\" layer to other types of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = intro.Network(\n",
    "    784, 64, 32, 10\n",
    ")\n",
    "\n",
    "losses = training.train(network, training_dataset, 40, batch_size=128)\n",
    "\n",
    "classification = training.classify(network, testing_dataset)\n",
    "\n",
    "print(classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.plot_loss_line(\n",
    "    [losses], ['Network'], 'Training Loss', save_file='images/t1x1_standard_network.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Straight-through layer network\n",
    "\n",
    "This network effectively implements a network that uses the straight-through estimator for training. This makes it possible to train hard activation functions by passing the gradient \"straight-through\" the hard activation function\n",
    "This can also be implemented directly with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = intro.STENetwork(\n",
    "    784, 64, 32, 10\n",
    ")\n",
    "\n",
    "losses = training.train(network, training_dataset, 40, batch_size=128)\n",
    "\n",
    "classification = training.classify(network, testing_dataset)\n",
    "print(classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.plot_loss_line(\n",
    "    [losses], ['STENetwork'], 'Training Loss', save_file='images/t1x1_ste_network.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop network\n",
    "\n",
    "This network uses layers that have looping for step and step_x. It executes step() before it executes step_x().\n",
    "\n",
    "For updating parameters\n",
    "\n",
    "- divide the minibatch into sub minibatches for each layer\n",
    "- do multiple epochs on each individual layer\n",
    "\n",
    "For computing the targets of the previous layer, it does multiple loops.\n",
    "\n",
    "One probably would not want to implement this by layer, but in some cases, you may wish to have large minibatches that you train the whole network over on one pass and then for some of the layers divide that into sub-minibatches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = intro.LoopNetwork(\n",
    "    784, 64, 32, 10\n",
    ")\n",
    "\n",
    "losses = training.train(network, training_dataset, 40, batch_size=2048)\n",
    "\n",
    "classification = training.classify(network, testing_dataset)\n",
    "\n",
    "print(classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.plot_loss_line(\n",
    "    [losses], ['Loop Network'], 'Training Loss', save_file='images/t1x1_loop_network.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
